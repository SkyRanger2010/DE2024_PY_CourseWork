name: de2024_py_coursework

x-postgresql-connection-env: &pg-connect
  POSTGRESQL_APP_HOST: ${POSTGRESQL_APP_HOST}
  POSTGRESQL_APP_DB: ${POSTGRESQL_APP_DB}
  POSTGRESQL_APP_SCHEMA: ${POSTGRESQL_APP_SCHEMA}
  POSTGRESQL_APP_USER: ${POSTGRESQL_APP_USER}
  POSTGRESQL_APP_PASSWORD: ${POSTGRESQL_APP_PASSWORD}

x-mysql-connection-env: &mysql-connect
  MYSQL_APP_HOST: ${MYSQL_APP_HOST}
  MYSQL_APP_DB: ${MYSQL_APP_DB}
  MYSQL_APP_USER: ${MYSQL_APP_USER}
  MYSQL_APP_PASSWORD: ${MYSQL_APP_PASSWORD}

x-airflow-common-env: &airflow-common-env
  AIRFLOW__CORE__EXECUTOR: ${AIRFLOW__CORE__EXECUTOR}
  AIRFLOW__CORE__LOAD_EXAMPLES: ${AIRFLOW__CORE__LOAD_EXAMPLES}
  AIRFLOW__CORE__DAG_CONCURRENCY: ${AIRFLOW__CORE__DAG_CONCURRENCY}
  AIRFLOW__DATABASE__SQL_ALCHEMY_CONN: ${AIRFLOW__DATABASE__SQL_ALCHEMY_CONN}
  AIRFLOW__SCHEDULER__ENABLE_HEALTH_CHECK: ${AIRFLOW__SCHEDULER__ENABLE_HEALTH_CHECK}
  AIRFLOW__WEBSERVER__SECRET_KEY: ${AIRFLOW__WEBSERVER__SECRET_KEY}
  AIRFLOW_UID: ${AIRFLOW_UID}
  _AIRFLOW_WWW_USER_USERNAME: ${_AIRFLOW_WWW_USER_USERNAME}
  _AIRFLOW_WWW_USER_PASSWORD: ${_AIRFLOW_WWW_USER_PASSWORD}
  SPARK_MASTER_HOST: ${SPARK_MASTER_HOST}
  SPARK_MASTER_PORT: ${SPARK_MASTER_PORT}
  
x-spark-common-env: &spark-common-env
  SPARK_RPC_AUTHENTICATION_ENABLED: ${SPARK_RPC_AUTHENTICATION_ENABLED}
  SPARK_RPC_ENCRYPTION_ENABLED: ${SPARK_RPC_ENCRYPTION_ENABLED}
  SPARK_LOCAL_STORAGE_ENCRYPTION_ENABLED: ${SPARK_LOCAL_STORAGE_ENCRYPTION_ENABLED}
  SPARK_SSL_ENABLED: ${SPARK_SSL_ENABLED}
  SPARK_WORKERS: ${SPARK_WORKERS}
  SPARK_DRIVER_MEMORY: ${SPARK_DRIVER_MEMORY}
  SPARK_EXECUTOR_MEMORY: ${SPARK_EXECUTOR_MEMORY}
  SPARK_WORKER_CORES: ${SPARK_WORKER_CORES}
  SPARK_WORKER_MEMORY: ${SPARK_WORKER_MEMORY}

x-kafka-common-env: &kafka-common-env
  KAFKA_INTERNAL_CONNECT_PATH: ${KAFKA_INTERNAL_CONNECT_PATH}
  KAFKA_USER_TOPIC_NAME: ${KAFKA_USER_TOPIC_NAME}
  KAFKA_REVIEW_TOPIC_NAME: ${KAFKA_REVIEW_TOPIC_NAME}


services:
  # RDBMS
  postgresql:
    build: docker-containers/db/postgresql
    container_name: postgresql
    environment:
      <<: *pg-connect
      # Пароль от root
      POSTGRES_PASSWORD: ${POSTGRESQL_ROOT_PASSWORD}
    ports:
      # Для просмотра данных с localhost
      - "5432:5432"
    volumes:
      - postgresql-data:/var/lib/postgres/data
    healthcheck:
      # condition, на основе которого проверяем готовность СУБД
      test: ["CMD-SHELL", "pg_isready -U postgres"]
      interval: 5s
      timeout: 5s
      retries: 5
    deploy:
      resources:
        limits:
          cpus: '0.5'
          memory: 512M

  mysql:
    build: docker-containers/db/mysql
    container_name: mysql
    environment:
      <<: *mysql-connect
      # Пароль от root
      MYSQL_ROOT_PASSWORD: ${MYSQL_ROOT_PASSWORD}
    ports:
      # Для просмотра данных с localhost
      - "3306:3306"
    volumes:
      - mysql-data:/var/lib/mysql
    healthcheck:
      # condition, на основе которого проверяем готовность СУБД
      test: ["CMD-SHELL", "mysqladmin ping -h localhost"]
      interval: 5s
      timeout: 5s
      retries: 5
    deploy:
      resources:
        limits:
          cpus: '0.5'
          memory: 512M

  # ZK & Kafka
  zookeeper:
    build: docker-containers/messaging/zookeeper
    container_name: zookeeper
    environment:
      ZOOKEEPER_CLIENT_PORT: '2181'
    ports:
      - "2181:2181"
    healthcheck:
      # condition, на основе которого проверяем готовность ZK
      test: ["CMD", "echo", "ruok", "|", "nc", "localhost", "2181"]
      interval: 5s
      timeout: 5s
      retries: 5
    deploy:
      resources:
        limits:
          cpus: '0.5'
          memory: 512M

  kafka:
    build: docker-containers/messaging/kafka
    hostname: kafka
    container_name: kafka
    depends_on:
      zookeeper:
        condition: service_healthy
    ports:
      - "29092:9092"
    environment:
      <<: *kafka-common-env
      KAFKA_BROKER_ID: ${KAFKA_BROKER_ID}
      KAFKA_ZOOKEEPER_CONNECT: ${KAFKA_ZOOKEEPER_CONNECT}
      KAFKA_LISTENER_SECURITY_PROTOCOL_MAP: ${KAFKA_LISTENER_SECURITY_PROTOCOL_MAP}
      KAFKA_ADVERTISED_LISTENERS: ${KAFKA_ADVERTISED_LISTENERS}
      KAFKA_OFFSETS_TOPIC_REPLICATION_FACTOR: ${KAFKA_OFFSETS_TOPIC_REPLICATION_FACTOR}
      KAFKA_GROUP_INITIAL_REBALANCE_DELAY_MS: ${KAFKA_GROUP_INITIAL_REBALANCE_DELAY_MS}
    healthcheck:
      # condition, на основе которого проверяем готовность Kafka
      test: ["CMD", "kafka-broker-api-versions", "--bootstrap-server", "localhost:9092"]
      interval: 5s
      timeout: 30s
      retries: 5
    deploy:
      resources:
        limits:
          cpus: '0.5'
          memory: 512M

  kafka-consumer:
    build: docker-containers/messaging/kafka-consumer
    container_name: kafka-consumer
    depends_on:
      kafka:
        condition: service_healthy
      postgresql:
        condition: service_healthy
      kafka-producer:
        condition: service_started
    environment:
      <<: [*kafka-common-env, *pg-connect]
      KAFKA_CONSUMER_GROUP: ${KAFKA_CONSUMER_GROUP}
      KAFKA_CONSUMER_AUTO_OFFSET_RESET: ${KAFKA_CONSUMER_AUTO_OFFSET_RESET}
    volumes:
      - ./python-scripts/kafka-consumer:/app/src
      - ./logs/kafka-consumer:/app/logs
    deploy:
      resources:
        limits:
          cpus: '0.5'
          memory: 512M


  kafka-init:
    build: docker-containers/messaging/kafka_init
    container_name: kafka-init
    depends_on:
      kafka:
        condition: service_healthy
    environment:
      <<: *kafka-common-env
    deploy:
        resources:
          limits:
            cpus: '0.5'
            memory: 512M
    
  # Spark
  spark-master:
    build: docker-containers/spark/spark-master
    container_name: spark-master
    environment:
      <<: *spark-common-env
      SPARK_MODE: master
      SPARK_MASTER_WEBUI_PORT: 8081
    ports:
      - "8081:8081"
    volumes:
      - spark-data:/opt/bitnami
      - ./logs/spark:/opt/bitnami/spark/logs
    healthcheck:
      test: ["CMD-SHELL", "curl -f http://localhost:8081"]
      interval: 10s
      timeout: 5s
      retries: 5
    deploy:
      resources:
        limits:
          cpus: '2'
          memory: 2G

  spark-worker:
    build: docker-containers/spark/spark-worker
    environment:
      <<: *spark-common-env
      SPARK_MODE: worker
      SPARK_MASTER_URL: spark://${SPARK_MASTER_HOST}:${SPARK_MASTER_PORT}
    volumes:
      - spark-data:/opt/bitnami
      - ./logs/spark:/opt/bitnami/spark/logs
    depends_on:
      spark-master:
        condition: service_healthy
    deploy:
      replicas: ${SPARK_WORKERS:-1}
      resources:
        limits:
          cpus: '2'
          memory: 2G


  # Datagen
  pg-datagen:
    build: docker-containers/datagen/pg_datagen
    container_name: pg-datagen
    depends_on:
      postgresql:
        condition: service_healthy
    environment:
      << : *pg-connect
      # Параметры генерации
      PG_DATAGEN_NUM_USERS: ${PG_DATAGEN_NUM_USERS}
      PG_DATAGEN_NUM_PRODUCTS: ${PG_DATAGEN_NUM_PRODUCTS}
      PG_DATAGEN_NUM_ORDERS: ${PG_DATAGEN_NUM_ORDERS}
      PG_DATAGEN_NUM_ORDER_DETAILS_MIN: ${PG_DATAGEN_NUM_ORDER_DETAILS_MIN}
      PG_DATAGEN_NUM_ORDER_DETAILS_MAX: ${PG_DATAGEN_NUM_ORDER_DETAILS_MAX}
      PG_DATAGEN_NUM_REVIEWS: ${PG_DATAGEN_NUM_REVIEWS}
      PG_DATAGEN_NUM_LOYALTY_POINTS: ${PG_DATAGEN_NUM_LOYALTY_POINTS}
    volumes:
      - ./python-scripts/datagen/datagen_postgres:/app/src
    deploy:
      resources:
        limits:
          cpus: '0.5'
          memory: 512M

  kafka-producer:
    build: docker-containers/datagen/kafka_producer
    container_name: kafka-producer
    depends_on:
      kafka-init:
        condition: service_completed_successfully
    environment:
      << : [*kafka-common-env, *pg-connect]
      # Параметры генерации
      KAFKA_DATAGEN_PERIOD_SECS: ${KAFKA_DATAGEN_PERIOD_SECS}
    volumes:
    - ./python-scripts/datagen/kafka_producer:/app/src
    deploy:
      resources:
        limits:
          cpus: '0.5'
          memory: 512M

  # Airflow
  airflow-init:
    build: docker-containers/airflow/init
    container_name: airflow-init
    depends_on:
      postgresql:
        condition: service_healthy
      mysql:
        condition: service_healthy
      kafka:
        condition: service_healthy
      spark-master:
        condition: service_healthy
      kafka-init:
        condition: service_completed_successfully
      pg-datagen:
        condition: service_completed_successfully
    environment:
      <<: [*airflow-common-env, *pg-connect, *mysql-connect]
    volumes:
    - ./python-scripts/airflow/dags:/opt/airflow/dags
    - ./python-scripts/airflow/scripts:/opt/airflow/scripts
    - ./logs/airflow:/opt/airflow/logs
    deploy:
      resources:
        limits:
          cpus: '0.5'
          memory: 512M

  airflow-scheduler:
    build: docker-containers/airflow/scheduler
    container_name: airflow-scheduler
    environment:
      <<: [*airflow-common-env, *kafka-common-env]
    restart: always
    depends_on:
      airflow-init:
        condition: service_completed_successfully
    volumes:
    - ./python-scripts/airflow/dags:/opt/airflow/dags
    - ./python-scripts/airflow/scripts:/opt/airflow/scripts
    - ./logs/airflow:/opt/airflow/logs
    deploy:
      resources:
        limits:
          cpus: '1'
          memory: 1G

  airflow-webserver:
    build: docker-containers/airflow/webserver
    container_name: airflow-webserver
    environment:
      <<: *airflow-common-env
    ports:
      - "8080:8080"
    restart: always
    depends_on:
      airflow-init:
        condition: service_completed_successfully
    volumes:
    - ./python-scripts/airflow/dags:/opt/airflow/dags
    - ./python-scripts/airflow/scripts:/opt/airflow/scripts
    - ./logs/airflow:/opt/airflow/logs
    deploy:
      resources:
        limits:
          cpus: '1'
          memory: 1G
  # Контейнер для разработки на Python + JupyterNotebooks внутри Docker-Compose
  dev:
    build: docker-containers/python-dev
    container_name: python-dev
    environment:
        <<: [ *pg-connect, *mysql-connect, *airflow-common-env, *spark-common-env, *kafka-common-env ]
        PYSPARK_PYTHON: python  # Указываем интерпретатор Python для Spark
        PYSPARK_DRIVER_PYTHON: python
    ports:
        - "9100:8000"  # Порт для разработки
        - "8888:8888"  # Порт для Jupyter
    volumes:
        - ./python-scripts/python-dev:/app  # Основная директория кода
        - ./logs/python-dev:/app/logs  # Логи разработки
        - ./python-scripts/python-dev/notebooks:/app/notebooks  # Для Jupyter notebooks
    depends_on:
        postgresql:
          condition: service_started
        mysql:
          condition: service_started
        kafka:
          condition: service_started
        spark-master:
          condition: service_started
        airflow-webserver:
          condition: service_started
    stdin_open: true
    tty: true
    command: [ "jupyter", "lab", "--ip=0.0.0.0", "--port=8888", "--no-browser", "--allow-root" ]

volumes:
  postgresql-data:
  mysql-data:
  spark-data: