# Итоговое домашнее задание по дисциплине "Python для инженерии данных"

Проект представляет собой ***полностью автоматизированную*** систему обработки данных, разворачиваемую ***одной командой***. Конфигурация проекта облегчена для запуска на локальной машине.

### Стек
- **Airflow**: Оркестрация.
- **Spark**: Обработка данных.
- **PostgreSQL** и **MySQL**: Реляционные базы данных.
- **Kafka** : Брокер сообщений.
- **Python**: Генераторы данных в PG и Kafka.
- **Jupyter Notebook**: Удобная работа с сервисами изнутри контейнера
- **Docker**: Контейнеризация сервисов.


## Автоматизация

Система состоит из 14 контейнеров:
1. PostgreSQL - Исходная БД
2. MySQL - Целевая БД с витринами
3. Spark Master - основной обработчик данных для репликации и создании витрин
4. Spark Worker - количество настраивается в конфигурационном файле
5. Генератор данных для PostgreSQL (pg_datagen)
6. Kafka Init - инициализатор Kafka (создание топиков)
7. Kafka - брокер сообщений
8. Kafka-Producer - Генератор сообщений в Kafka (эмуляция регистраций пользователей и добавления отзывов на товары)
9. Kafka-Consumer - Обработчик сообщений Kafka - Добавление данных в исходную БД
10. Zookeeper
11. Airflow Init - Инициализация AW (Создание пользователя + соединений)
12. Airflow Scheduler 
13. Airflow Webserver
14. Dev - Python + Jupyter Notebook для работы внутри контейнера


Все контейнеры основаны на открытых Docker Image. Они автоматически настраиваются и проверяют свою готовность с использованием Healthcheck. Система обеспечена скриптами для автоматической генерации данных, их репликации, стриминга и создания аналитических витрин.



### Сущности исходной базы данных:

- **users**: Информация о пользователях.
  - `user_id` (PK): Уникальный идентификатор пользователя.
  - `first_name`: Имя пользователя.
  - `last_name`: Фамилия пользователя.
  - `email`: Электронная почта (уникальное значение).
  - `phone`: Номер телефона.
  - `registration_date`: Дата регистрации.
  - `loyalty_status`: Статус лояльности (`Gold`, `Silver`, `Bronze`).

- **productcategories**: Иерархия категорий товаров.
  - `category_id` (PK): Уникальный идентификатор категории.
  - `name`: Название категории.
  - `parent_category_id` (FK): Ссылка на родительскую категорию.

- **products**: Информация о товарах.
  - `product_id` (PK): Уникальный идентификатор товара.
  - `name`: Название товара.
  - `description`: Описание товара.
  - `category_id` (FK): Категория товара.
  - `price`: Цена товара.
  - `stock_quantity`: Количество товара на складе.
  - `creation_date`: Дата добавления товара.

- **orders**: Информация о заказах.
  - `order_id` (PK): Уникальный идентификатор заказа.
  - `user_id` (FK): Пользователь, сделавший заказ.
  - `order_date`: Дата заказа.
  - `total_amount`: Общая сумма заказа.
  - `status`: Статус заказа (`Pending`, `Completed`, и т.д.).
  - `delivery_date`: Дата доставки.

- **orderdetails**: Детали заказов.
  - `order_detail_id` (PK): Уникальный идентификатор детали заказа.
  - `order_id` (FK): Ссылка на заказ.
  - `product_id` (FK): Ссылка на товар.
  - `quantity`: Количество товаров в заказе.
  - `price_per_unit`: Цена за единицу товара.
  - `total_price`: Общая стоимость позиции.

- **reviews**: Отзывы о товарах.
  - `review_id` (PK): Уникальный идентификатор отзыва.
  - `user_id` (FK): Пользователь, оставивший отзыв.
  - `product_id` (FK): Продукт, на который оставлен отзыв.
  - `rating`: Оценка товара (от 1 до 5).
  - `review_text`: Текст отзыва.
  - `created_at`: Дата создания отзыва.

- **loyaltyPoints**: Система лояльности.
  - `loyalty_id` (PK): Уникальный идентификатор записи.
  - `user_id` (FK): Пользователь, получивший бонусные баллы.
  - `points`: Количество начисленных баллов.
  - `reason`: Причина начисления (например, "Order", "Promotion").
  - `created_at`: Дата начисления.
 
    
## Сервисы и их конфигурация

### Управление конфигурацией
Вся конфигурация проекта управляется через единый файл `.env`, который содержит параметры для всех сервисов, включая логины, пароли, порты и настройки генерации данных.

### Доступы к данным и UI сервисов

### PostgreSQL
- **URL**: `jdbc:postgresql://localhost:5432/postgres_db?currentSchema=source`
- **Логин**: `db_user`
- **Пароль**: `qwerty`

### MySQL
- **URL**: `jdbc:mysql://localhost:3306/mysql_db`
- **Логин**: `db_user`
- **Пароль**: `qwerty`

### Airflow Web UI
- **URL**: `http://localhost:8080`
- **Логин**: `admin`
- **Пароль**: `admin`

### Spark Master
- **URL**: `http://localhost:8081`

### Kafka
- **Bootstrap Servers**: `localhost:9092`
- **Топики**: `new_user_events`, `new_review_events`

### Jupyter Notebook
- **URL**: `http://localhost:8888`
  для корректного подключения используйте ссылку в выводе собранного контейнера python-dev, в которой указан токен подключения
- **Примеры Notebooks**: расположены в файлах сборки, которые будут доступны из Jupyter


### Генерация данных

**Параметры генерации для PostgreSQL по умолчанию:**
  - Количество пользователей: 500
  - Количество товаров: 800
  - Количество заказов: 3000
  - Детали заказов: от 1 до 10 на заказ
  - Категории товаров: 20
  - Отзывов о товарах: 2000
  - Начислений бонусов: 3000
    * Внутри БД Postgresql реализовано логирование добавления новых записей через триггеры и триггерные функции с дополнительным признаком 
    для уже обработанных строк лога. Это позволяет выполнять инкрементальную репликацию.

**Параметры генерации для Kafka по умолчанию:**
  - Интервал генерации событий: 5 секунды
  - Топик Kafka: `new_user_events`
     * События генерируются в формате JSON с полями: `first_name`, `last_name`, `email`, `phone`, `registration_date`, `loyalty_status`.
  - Топик Kafka: `new_review_events`
       * События генерируются в формате JSON с полями: `user_id`, `product_id`, `rating`, `review_text`, `created_at`
       * `user_id`, `product_id` для генерации выбираются случайно из существующих в целевой базе
    
## Репликация данных

В Airflow реализована репликация данных из PostgreSQL в MySQL. DAG выполняет следующие задачи:
1. Инкрементальное извлечение данных из PostgreSQL (только тех строк, которые были добавлены после последней репликации).
2. Трансформация данных через Spark.
3. Сохранение данных в MySQL.

## Стриминг данных
Отдельным сервисом Kafka-Consumer реализована обработка данных из Kafka. 
1. Получение данных из топиков Kafka.
2. Обработка и сохранение данных в PostgreSQL.

## Аналитические витрины

Каждая витрина формируется отдельным DUG в AirFlow, для удобства запуска по отдельности

### Витрина активности пользователей (mart_user_activity)

#### Описание
Витрина для последующего анализа поведения пользователей - количества заказов, общей суммы затрат, количеством отзывов и средней оценкой товаров пользователем.

#### Поля и их описание

| Поле           | Описание                             |
|----------------|--------------------------------------|
| user_id        | Идентификатор пользователя           |
| first_name     | Имя пользователя                     |
| last_name      | Фамилия пользователя                 |
| order_count    | Количество заказов                   |
| total_spent    | Общая сумма затрат                   |
| total_reviews  | Количество отзывов пользователя      |
| average_rating | Средняя оценка товаров пользователем | 

### Витрина рейтинга продуктов (mart_products_rating)

#### Описание
Витрина для последующего анализа рейтинга товаров. Может быть использована, например, для исследования популярности товаров и их соответствия ожиданиям клиентов.

#### Поля и их описание
| Поле           | Описание                          |
|----------------|-----------------------------------|
| product_id     | Идентификатор товара              |
| name           | Название товара                   |
| category_name  | Категория товара                  |
| average_rating | Средний рейтинг                   |
| total_reviews  | Общее количество отзывов о товаре |


### Витрина средних чеков (mart_average_check)

#### Описание
Витрина для последующего анализа средних чеков с разбивкой по статусу заказа и статусу лояльности. Может быть использована, например, для оценки эффективностм маркетинговых стратегий и программ лояльности, или для настройки акций и предложений для разных групп клиентов.

#### Поля и их описание
| Поле           | Описание                               |
|----------------|----------------------------------------|
| order_status   | Статус заказа                          |
| loyalty_status | Статус лояльности пользователя         |
| average_check  | Средний чек для группы заказов         |


### Скрипты для витрин

Для создания витрин используется Spark. Скрипты загружают данные из MySQL, выполняют агрегации и сохраняют результаты обратно в базу данных. Шаги:
1. Загрузка исходных данных из базы данных MySQL.
2. Выполнение трансформаций (объединения, группировки, агрегации, etc.)
3. Сохранение результирующих витрин в базу данных MySQL.

## Запуск

1. Запустить команду:
    ```
    docker compose up --build [-d]
    ```
2. После сборки проекта и его развертывания будут доступны интерфейсы PostgreSQL, MySQL, Airflow, Kafka, Spark и Jupyter Notebook по указанным выше URL.
3. Все что остается сделать вручную после окончания деплоя - включить (переевсти в `unpaused`) DAG в UI Airflow. 


## Структура проекта
Проект организован следующим образом:
```plaintext
DE2024_PY_CourseWork/
├── .env                      # Переменные окружения для настройки всех сервисов
├── docker-compose.yml        # Конфигурация Docker Compose
├── docker-containers         # Инфраструктура контейнеров
│   ├── airflow/              # Конфигурация Airflow
│   │   ├── init/       
│   │   ├── scheduler/  
│   │   └── webserver/  
│   ├── datagen/              # Конфигурация для генераторов данных
│   │   ├── pg_datagen/
│   │   └── kafka_producer/
│   ├── db/                   # Конфигурация СУБД
│   │   ├── mysql/     
│   │   └── postgresql/
│   ├── messaging/            # Конфигурация ZK&Kafka
│   │   ├── kafka/
│   │   ├── kafka_consumer/
│   │   ├── kafka_init/
│   │   └── zookeeper/
│   ├── python-dev/           # Конфигурация dev контейнера
│   └── spark/                # Конфигурация Spark   
│       ├── spark-master/
│       └── spark-worker/
├── logs  # Логи контейнеров
└── python-scripts/           # Исходный код
    ├── airflow/              # DAG и скрипты для Airflow
    │   ├── dags/       
    │   └── scripts/    
    │       ├── helpers/        
    │       └── pyspark_scripts/ 
    ├── datagen/              # Генераторы данных
    │   ├── datagen_postgres/ # Скрипты для генерации данных в PG
    │   └── kafka_producer/   # Скрипты для генерации данных в Kafka
    ├── kafka-consumer/       # Скрипты Consumera Kafka
    └── python-dev/           # Скрипты Consumera Kafka
        ├── logs/ 
        └── notebooks/        # Примеры ноутбуков
```
